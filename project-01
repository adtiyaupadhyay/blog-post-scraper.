import xml.etree.ElementTree as ET
import requests
from urllib.parse import urljoin
from bs4 import BeautifulSoup
import csv

header = {
    'user-agent': 'https://explore.whatismybrowser.com/useragents/parse/911679-chrome-windows-blink'
}


def get_sitemap(url):
    try:
        robot_url = urljoin(url, '/robots.txt')
        response = requests.get(robot_url, headers=header)
        if response.status_code == 200:
            lines = response.text.splitlines()
            for line in lines:
                if line.lower().startswith('sitemap'):
                    return line.split(":", 1)[1].strip()
    except Exception as e:
        print(f"Error fetching robots.txt: {e}")

    return urljoin(url, "/sitemap.xml")


def get_sitemap_content(url):
    try:
        response = requests.get(url, headers=header)
        if response.status_code == 200:
            return response.text  
    except Exception as e:
        print(f"Error fetching sitemap: {e}")
    return None


def parse_sitemap(url):
    urls = []
    xml_content = get_sitemap_content(url)
    if xml_content:
        try:
            root = ET.fromstring(xml_content)

            if "sitemapindex" in root.tag:
                for sitemap in root.findall(".//{*}sitemap"):
                    loc_tag = sitemap.find("{*}loc")
                    if loc_tag is not None:
                        nested_sitemap_url = loc_tag.text
                        print(f"Found nested sitemap: {nested_sitemap_url}")
                        urls.extend(parse_sitemap(nested_sitemap_url))  
            elif "urlset" in root.tag:
                for url_tag in root.findall(".//{*}url"):
                    loc_tag = url_tag.find("{*}loc")
                    if loc_tag is not None:
                        url = loc_tag.text
                        if "/blog/" in url:  
                            urls.append(url)
        except ET.ParseError as e:
            print(f"Error parsing sitemap: {e}")
    return urls


def get_title(url):
    try:
        response = requests.get(url, headers=header)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            title_tag = soup.find("title")
            return title_tag.text.strip() if title_tag else "No title Found"
    except Exception as e:
        print(f"Error fetching title for {url}: {e}")
    return "No title Found"


def scrap_blog_posts(url):
    sitemap_url = get_sitemap(url)
    print(f"Using sitemap: {sitemap_url}")

    blog_post_urls = parse_sitemap(sitemap_url)
    if not blog_post_urls:
        print("No blog post URLs found.")
        return

    with open('blog_posts.csv', mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(["URL", "TITLE"])

        for url in blog_post_urls:
            title = get_title(url)
            print(f"Fetched {url} --> {title}")
            writer.writerow([url, title])

    print("All blog posts saved to blog_posts.csv!")


# Run the scraper
if __name__ == "__main__":
    website_url = input("Enter website URL (e.g., https://example.com): ")
    scrap_blog_posts(website_url)
